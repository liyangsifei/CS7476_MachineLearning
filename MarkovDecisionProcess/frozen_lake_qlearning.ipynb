{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')\n",
    "\n",
    "n_state = env.nS\n",
    "n_action = env.nA\n",
    "\n",
    "print(n_state)\n",
    "print(n_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(param, curves, param_name=\"\", curve_name=\"\"):\n",
    "    colors = ['deepskyblue', 'red', 'yellow', 'green', 'midnightblue', 'fuchsia']\n",
    "    plt.figure()\n",
    "    lines = []\n",
    "    l, = plt.plot(param, curves, ls='-', marker='+', color='deepskyblue')\n",
    "    lines.append(l)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(curve_name)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(param, curves, curve_labels, param_name=\"\", curve_name=\"\"):\n",
    "    colors = ['firebrick', 'cadetblue', 'darksalmon', 'mediumseagreen', 'darkmagenta', 'skyblue', \n",
    "              'gold','palevioletred', 'olive', 'darkorange', 'mediumpurple', 'slategray', 'darkseagreen']\n",
    "    plt.figure()\n",
    "    lines = []\n",
    "    for i in range(len(curve_labels)):\n",
    "        l, = plt.plot(param, curves[i,:], ls='-', marker='', color=colors[i])\n",
    "        lines.append(l)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(curve_name)\n",
    "    \n",
    "    plt.legend(handles=lines, labels=curve_labels, loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(Q, episodes, to_print=False):\n",
    "    misses = 0\n",
    "    steps_list = []\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            next_state, rew, done, _ = env.step(greedy(Q, state))\n",
    "            \n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            if done and rew == 1:\n",
    "                step_list.append(steps)\n",
    "                break\n",
    "            elif done and rew == 0:\n",
    "                misses += 1\n",
    "                break\n",
    "    mean_steps = np.mean(steps_list)\n",
    "    per_misses = (misses/episodes) * 100\n",
    "    if to_print:\n",
    "        print('Mean steps: %.3f . Percentage %.2f of games lost!' % (mean_steps, per_misses))\n",
    "    else:\n",
    "        return mean_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9):\n",
    "        self.env = env\n",
    "        self.episode_reward = 0.0\n",
    "        self.turn_limit = 100\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.q_val = np.zeros(n_state * n_action).reshape(n_state, n_action).astype(np.float32)\n",
    "        self.q_val_rand_grd = np.zeros(n_state * n_action).reshape(n_state, n_action).astype(np.float32)\n",
    "        self.q_val_greedy = np.zeros(n_state * n_action).reshape(n_state, n_action).astype(np.float32)\n",
    "        self.eps = 0.33\n",
    "        self.eps_decay = 0.00005\n",
    "        self.start_time = 0\n",
    "    \n",
    "    def learn(self, alpha, gamma):\n",
    "        state = self.env.reset()\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        # for t in range(self.turn_limit):\n",
    "        while True:\n",
    "            act = self.env.action_space.sample()\n",
    "            next_state, reward, done, info = self.env.step(act)\n",
    "            q_next_max = np.max(self.q_val[next_state])\n",
    "            \n",
    "            self.q_val[state][act] = (1 - alpha) * self.q_val[state][act] + alpha * (reward + gamma * q_next_max)\n",
    "\n",
    "            if done:\n",
    "                return reward\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "                \n",
    "    def learn_rg(self, alpha, gamma):\n",
    "        state = self.env.reset()\n",
    "        self.start_time = time.time()\n",
    "        self.eps = 0.33\n",
    "        \n",
    "        while True:\n",
    "            rand = np.random.uniform(0,1)\n",
    "            if rand < self.eps:\n",
    "                act = self.env.action_space.sample()\n",
    "            else:\n",
    "                act = np.argmax(self.q_val_greedy[state])\n",
    "                \n",
    "            next_state, reward, done, info = self.env.step(act)\n",
    "            q_next_max = np.max(self.q_val[next_state])\n",
    "            \n",
    "            self.q_val[state][act] = (1 - alpha) * self.q_val[state][act] + alpha * (reward + gamma * q_next_max)\n",
    "            \n",
    "            if self.eps > 0.01:\n",
    "                self.eps -= self.eps_decay\n",
    "                \n",
    "            if done:\n",
    "                return reward\n",
    "            else:\n",
    "                state = next_state\n",
    "                \n",
    "    \n",
    "    def learn_greedy(self, alpha, gamma):\n",
    "        state = self.env.reset()\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            act = np.argmax(self.q_val_greedy[state])\n",
    "                \n",
    "            next_state, reward, done, info = self.env.step(act)\n",
    "            q_next_max = np.max(self.q_val_greedy[next_state])\n",
    "            \n",
    "            self.q_val_greedy[state][act] = (1 - alpha) * self.q_val_greedy[state][act] + alpha * (\n",
    "                reward + gamma * q_next_max)\n",
    "            if done:\n",
    "                return reward\n",
    "            else:\n",
    "                state = next_state\n",
    "                \n",
    "    def test(self):\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        for t in range(self.turn_limit):\n",
    "            act = np.argmax(self.q_val[state])\n",
    "            next_state, reward, done, _ = self.env.step(act)\n",
    "            \n",
    "            if done:\n",
    "                return reward\n",
    "            else:\n",
    "                state = next_state\n",
    "        return 0.0\n",
    "    \n",
    "    def test_random_greedy(self):\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        for t in range(self.turn_limit):\n",
    "            act = np.argmax(self.q_val_rand_grd[state])\n",
    "            next_state, reward, done, _ = self.env.step(act)\n",
    "            \n",
    "            if done:\n",
    "                return reward\n",
    "            else:\n",
    "                state = next_state\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "    def test_greedy(self):\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        for t in range(self.turn_limit):\n",
    "            act = np.argmax(self.q_val_greedy[state])\n",
    "            next_state, reward, done, _ = self.env.step(act)\n",
    "            \n",
    "            if done:\n",
    "                return reward\n",
    "            else:\n",
    "                state = next_state\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d68dbc5b35e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mavg_rew_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepi_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mreward_tot\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtot_rew_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_tot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mavg_rew_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_tot\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-84ef75b97b93>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, alpha, gamma)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mq_next_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_next_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reward_tot = 0.0\n",
    "epi_train = list(range(1000000))\n",
    "tot_rew_list = []\n",
    "avg_rew_list = []\n",
    "for i in epi_train:\n",
    "    reward_tot += agent.learn(alpha=0.1, gamma=0.9)\n",
    "    tot_rew_list.append(reward_tot)\n",
    "    avg_rew_list.append(reward_tot/(i+1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_test = 0.0\n",
    "epi_test = list(range(1000))\n",
    "tot_test_rew_list = []\n",
    "avg_test_rew_list = []\n",
    "for i in epi_test:\n",
    "    reward_test += agent.test()\n",
    "    tot_test_rew_list.append(reward_test)\n",
    "    avg_test_rew_list.append(reward_test/(i+1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(epi_train, tot_rew_list, param_name=\"Q-Learning episodes\", curve_name=\"total reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_graph(epi_train, avg_rew_list, param_name=\"Q-Learning episodes\", curve_name=\"average reward %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_graph(epi_test, tot_test_rew_list, param_name=\"Q-Learning test episodes\", curve_name=\"total reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_graph(epi_test, avg_test_rew_list, param_name=\"Q-Learning test episodes\", curve_name=\"average reward %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_tot_rand_grd = 0.0\n",
    "epi_train = list(range(100000))\n",
    "tot_rew_list_rand_grd = []\n",
    "avg_rew_list_rand_grd = []\n",
    "for i in epi_train:\n",
    "    reward_tot_rand_grd += agent.learn_rg(alpha=0.5, gamma=0.9)\n",
    "    tot_rew_list_rand_grd.append(reward_tot_rand_grd)\n",
    "    avg_rew_list_rand_grd.append(reward_tot_rand_grd/(i+1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_test_rand_grd = 0.0\n",
    "epi_test = list(range(1000))\n",
    "tot_test_rew_list_rand_grd = []\n",
    "avg_test_rew_list_rand_grd = []\n",
    "for i in epi_test:\n",
    "    reward_test_rand_grd += agent.test_random_greedy()\n",
    "    tot_test_rew_list_rand_grd.append(reward_test_rand_grd)\n",
    "    avg_test_rew_list_rand_grd.append(reward_test_rand_grd/(i+1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(epi_train, tot_rew_list_rand_grd, param_name=\"Q-Learning episodes\", curve_name=\"total reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_graph(epi_train, avg_rew_list_rand_grd, param_name=\"Q-Learning episodes\", curve_name=\"average reward %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_graph(epi_test, tot_test_rew_list_rand_grd, param_name=\"Q-Learning test episodes\", curve_name=\"total reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_graph(epi_test, avg_test_rew_list_rand_grd, param_name=\"Q-Learning test episodes\", curve_name=\"average reward %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_tot_greedy = 0.0\n",
    "epi_train = list(range(1000000))\n",
    "tot_rew_list_greedy = []\n",
    "avg_rew_list_greedy = []\n",
    "for i in epi_train:\n",
    "    reward_tot_greedy += agent.learn_greedy(alpha=0.1, gamma=0.9)\n",
    "    tot_rew_list_greedy.append(reward_tot_greedy)\n",
    "    avg_rew_list_greedy.append(reward_tot_greedy/(i+1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_test_greedy = 0.0\n",
    "epi_test = list(range(1000))\n",
    "tot_test_rew_list_greedy = []\n",
    "avg_test_rew_list_greedy = []\n",
    "for i in epi_test:\n",
    "    reward_test_greedy += agent.test_greedy()\n",
    "    tot_test_rew_list_greedy.append(reward_test_greedy)\n",
    "    avg_test_rew_list_greedy.append(reward_test_greedy/(i+1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(epi_train, tot_rew_list_greedy, param_name=\"Q-Learning episodes\", curve_name=\"total reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_graph(epi_train, avg_rew_list_greedy, param_name=\"Q-Learning episodes\", curve_name=\"average reward %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_graph(epi_test, tot_test_rew_list_greedy, param_name=\"Q-Learning test episodes\", curve_name=\"total reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_graph(epi_test, avg_test_rew_list_greedy, param_name=\"Q-Learning test episodes\", curve_name=\"average reward %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
